/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)
Kokkos::OpenMP::initialize WARNING: You are likely oversubscribing your CPU cores.
                                    Detected: 64 cores per node.
                                    Detected: 6 MPI_ranks per node.
                                    Requested: 12 threads per process.
Kokkos::OpenMP::initialize WARNING: You are likely oversubscribing your CPU cores.
                                    Detected: 64 cores per node.
                                    Detected: 6 MPI_ranks per node.
                                    Requested: 12 threads per process.
Kokkos::OpenMP::initialize WARNING: You are likely oversubscribing your CPU cores.
                                    Detected: 64 cores per node.
                                    Detected: 6 MPI_ranks per node.
                                    Requested: 12 threads per process.
Initializing EulerMultiD
Kokkos::OpenMP::initialize WARNING: You are likely oversubscribing your CPU cores.
                                    Detected: 64 cores per node.
                                    Detected: 6 MPI_ranks per node.
                                    Requested: 12 threads per process.
Kokkos::OpenMP::initialize WARNING: You are likely oversubscribing your CPU cores.
                                    Detected: 64 cores per node.
                                    Detected: 6 MPI_ranks per node.
                                    Requested: 12 threads per process.
Kokkos::OpenMP::initialize WARNING: You are likely oversubscribing your CPU cores.
                                    Detected: 64 cores per node.
                                    Detected: 6 MPI_ranks per node.
                                    Requested: 12 threads per process.
[09:35:51][MPI 000010][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000005][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000015][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000013][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000002][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000006][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000011][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000004][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000003][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000007][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000000][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000012][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000001][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000009][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000014][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000008][MPI] *** info: Plugin loaded successfully
[09:35:51][MPI 000005][PDI] *** info: Initialization successful
[09:35:51][MPI 000011][PDI] *** info: Initialization successful
[09:35:51][MPI 000010][PDI] *** info: Initialization successful
[09:35:51][MPI 000004][PDI] *** info: Initialization successful
[09:35:51][MPI 000015][PDI] *** info: Initialization successful
[09:35:51][MPI 000006][PDI] *** info: Initialization successful
[09:35:51][MPI 000013][PDI] *** info: Initialization successful
[09:35:51][MPI 000000][PDI] *** info: Initialization successful
[09:35:51][MPI 000007][PDI] *** info: Initialization successful
[09:35:51][MPI 000003][PDI] *** info: Initialization successful
[09:35:51][MPI 000002][PDI] *** info: Initialization successful
[09:35:51][MPI 000012][PDI] *** info: Initialization successful
[09:35:51][MPI 000001][PDI] *** info: Initialization successful
[09:35:51][MPI 000009][PDI] *** info: Initialization successful
[09:35:51][MPI 000014][PDI] *** info: Initialization successful
[09:35:51][MPI 000008][PDI] *** info: Initialization successful
Initializing Convection problem... done
################################################################################
############################## Run parameters ##################################
solver...................................................................godunov
riemann..........................................................MHD3W_optimized
cfl........................................................9.000000000000000e-01
tEnd.......................................................1.000000000000000e+01
slope_type.................................................1.000000000000000e+00
muscl_enabled..................................................................1
all_regime_correction..........................................................1
all_regime_correction_type.....................................................0
nStepmax.....................................................................500
info.........................................................................100
restart........................................................................0
restart_filename................................................................
powell_st_when_low_plasma_beta :...............................................0
beta_threshold :..........................................-1.000000000000000e+00
random_perturbation :..........................................................1
three_d_perturbation :.........................................................1
############################## Hydro parameters ################################
convection_source_term_enabled.................................................1
H_source_term..................................................................1
R_source_term..................................................................1
Q_source_term..................................................................1
HT........................................................-1.200000000000000e+00
RX........................................................-6.000000000000000e-01
QA........................................................-1.000000000000000e-03
gx.........................................................0.000000000000000e+00
gy.........................................................0.000000000000000e+00
gz........................................................-1.000000000000000e+00
############################## Thermo parameters ###############################
Eos type.............................................................perfect gas
mmw........................................................1.000000000000000e+00
gamma......................................................1.400000000000000e+00
############################## Mesh parameters #################################
nx..........................................................................1024
ny...........................................................................512
nz............................................................................32
mx.............................................................................4
my.............................................................................4
mz.............................................................................1
xmin.......................................................0.000000000000000e+00
xmax.......................................................1.000000000000000e+00
ymin.......................................................0.000000000000000e+00
ymax.......................................................1.000000000000000e+00
zmin.......................................................0.000000000000000e+00
zmax.......................................................1.000000000000000e+00
boundary type xmin.............................................................3
boundary type xmax.............................................................3
boundary type ymin.............................................................3
boundary type ymax.............................................................3
boundary type zmin.............................................................0
boundary type zmax.............................................................0
############################## Output parameters ###############################
type.........................................................................pdi
format..................................................................appended
prefix........................................................../tmp/CheckPoints
nOutput........................................................................1
dt_io......................................................0.000000000000000e+00
################################################################################
memory requested [in Mo]...................................1.099936972800000e+10
Starting simulation from step n=........0; time t=0.000000000000000e+00
===================== output at iteration = 0 time t = 0
[10:22:06][MPI 000015][PDI] *** error: Unable to share `local_full_field', Unable to share `local_full_field', Error while triggering data share `local_full_field': Plugin_error: While publishing data. Caught exception: CommClosedError: in <TCP (closed) ConnectionPool.update_data local=tcp://172.16.20.22:58258 remote=tcp://172.16.20.22:35195>: ConnectionResetError: [Errno 104] Connection reset by peer

At:
  /usr/local/lib/python3.12/dist-packages/distributed/comm/tcp.py(140): convert_stream_closed_error
  /usr/local/lib/python3.12/dist-packages/distributed/comm/tcp.py(236): read
  /usr/local/lib/python3.12/dist-packages/distributed/core.py(1171): send_recv
  /usr/local/lib/python3.12/dist-packages/distributed/core.py(1398): send_recv_from_rpc
  /usr/lib/python3.12/asyncio/events.py(103): _run
  /usr/lib/python3.12/asyncio/base_events.py(1988): _run_once
  /usr/lib/python3.12/asyncio/base_events.py(641): run_forever
  /usr/lib/python3.12/asyncio/base_events.py(674): run_until_complete
  /usr/lib/python3.12/asyncio/runners.py(118): run
  /usr/lib/python3.12/asyncio/runners.py(194): run
  /usr/local/lib/python3.12/dist-packages/distributed/utils.py(583): run_loop
  /usr/local/lib/python3.12/dist-packages/distributed/utils.py(475): wrapper
  /usr/lib/python3.12/threading.py(1010): run
  /usr/lib/python3.12/threading.py(1073): _bootstrap_inner
  /usr/lib/python3.12/threading.py(1030): _bootstrap

[10:22:06][MPI 000003][PDI] *** error: Unable to share `local_full_field', Unable to share `local_full_field', Error while triggering data share `local_full_field': Plugin_error: While publishing data. Caught exception: CommClosedError: in <TCP (closed) ConnectionPool.update_data local=tcp://172.16.20.22:58240 remote=tcp://172.16.20.22:35195>: ConnectionResetError: [Errno 104] Connection reset by peer

At:
  /usr/local/lib/python3.12/dist-packages/distributed/comm/tcp.py(140): convert_stream_closed_error
  /usr/local/lib/python3.12/dist-packages/distributed/comm/tcp.py(236): read
  /usr/local/lib/python3.12/dist-packages/distributed/core.py(1171): send_recv
  /usr/local/lib/python3.12/dist-packages/distributed/core.py(1398): send_recv_from_rpc
  /usr/lib/python3.12/asyncio/events.py(103): _run
  /usr/lib/python3.12/asyncio/base_events.py(1988): _run_once
  /usr/lib/python3.12/asyncio/base_events.py(641): run_forever
  /usr/lib/python3.12/asyncio/base_events.py(674): run_until_complete
  /usr/lib/python3.12/asyncio/runners.py(118): run
  /usr/lib/python3.12/asyncio/runners.py(194): run
  /usr/local/lib/python3.12/dist-packages/distributed/utils.py(583): run_loop
  /usr/local/lib/python3.12/dist-packages/distributed/utils.py(475): wrapper
  /usr/lib/python3.12/threading.py(1010): run
  /usr/lib/python3.12/threading.py(1073): _bootstrap_inner
  /usr/lib/python3.12/threading.py(1030): _bootstrap

[10:22:06][MPI 000015][PDI] *** info: Finalization
[10:22:06][MPI 000015][deisa] *** info: Closing plugin
[10:22:06][MPI 000003][PDI] *** info: Finalization
[10:22:06][MPI 000003][deisa] *** info: Closing plugin
[10:22:08][MPI 000015][MPI] *** info: Closing plugin
[10:22:08][MPI 000015][PDI] *** warning: Remaining 1 reference(s) to `gamma' in PDI after program end
[10:22:08][MPI 000015][PDI] *** warning: Remaining 1 reference(s) to `mmw' in PDI after program end
[10:22:08][MPI 000015][PDI] *** warning: Remaining 1 reference(s) to `restart_id' in PDI after program end
[10:22:08][MPI 000015][PDI] *** warning: Remaining 1 reference(s) to `time' in PDI after program end
[10:22:08][MPI 000015][PDI] *** warning: Remaining 1 reference(s) to `iStep' in PDI after program end
[10:22:08][MPI 000015][PDI] *** warning: Remaining 1 reference(s) to `Rstar_h' in PDI after program end
[10:22:08][MPI 000015][PDI] *** warning: Remaining 1 reference(s) to `output_id' in PDI after program end
[10:22:08][MPI 000003][MPI] *** info: Closing plugin
[10:22:08][MPI 000003][PDI] *** warning: Remaining 1 reference(s) to `gamma' in PDI after program end
[10:22:08][MPI 000003][PDI] *** warning: Remaining 1 reference(s) to `mmw' in PDI after program end
[10:22:08][MPI 000003][PDI] *** warning: Remaining 1 reference(s) to `restart_id' in PDI after program end
[10:22:08][MPI 000003][PDI] *** warning: Remaining 1 reference(s) to `time' in PDI after program end
[10:22:08][MPI 000003][PDI] *** warning: Remaining 1 reference(s) to `iStep' in PDI after program end
[10:22:08][MPI 000003][PDI] *** warning: Remaining 1 reference(s) to `Rstar_h' in PDI after program end
[10:22:08][MPI 000003][PDI] *** warning: Remaining 1 reference(s) to `output_id' in PDI after program end
--------------------------------------------------------------------------
An MPI communication peer process has unexpectedly disconnected.  This
usually indicates a failure in the peer process (e.g., a crash or
otherwise exiting without calling MPI_FINALIZE first).

Although this local MPI process will likely now behave unpredictably
(it may even hang or crash), the root cause of this problem is the
failure of the peer -- that is what you need to investigate.  For
example, there may be a core file that you can examine.  More
generally: such peer hangups are frequently caused by application bugs
or other external events.

  Local host: dahu-9
  Local PID:  7263
  Peer host:  dahu-22
--------------------------------------------------------------------------
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[14608,1],15]
  Exit code:    4
--------------------------------------------------------------------------
[dahu-20.grenoble.grid5000.fr:50976] 5 more processes have sent help message help-mpi-btl-tcp.txt / peer hung up
[dahu-20.grenoble.grid5000.fr:50976] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
