{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ad10d-fcb5-41ce-8fd7-db8d62872dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-20 13:46:13,665 \u001b[36mWARNING:\u001b[m terminated: <SshProcess('singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"dask scheduler --host dahu-9.grenoble.grid5000.fr --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/scheduler.e 2>&1\"', Host('dahu-9.grenoble.grid5000.fr'), name=singularity exec /home/lmascare/benc..., started=True, start_date=2025-05-20 13:36:35+02:00, ended=True, end_date=2025-05-20 13:46:13+02:00, killed=False, error=False, error_reason=None, timeouted=False, expect_fail=False, write_error=False, exit_code=65280, ok=False, pid=3486719, real cmd=('ssh', '-tt', '-o', 'BatchMode=yes', '-o', 'PasswordAuthentication=no', '-o', 'StrictHostKeyChecking=no', '-o', 'UserKnownHostsFile=/dev/null', '-o', 'ConnectTimeout=20', 'dahu-9.grenoble.grid5000.fr', 'singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"dask scheduler --host dahu-9.grenoble.grid5000.fr --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/scheduler.e 2>&1\"'))>\n",
      "stdout:\n",
      "/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "\n",
      "stderr:\n",
      "Warning: Permanently added 'dahu-9.grenoble.grid5000.fr,172.16.20.9' (ECDSA) to the list of known hosts.\n",
      "Connection to dahu-9.grenoble.grid5000.fr closed by remote host.\n",
      "Connection to dahu-9.grenoble.grid5000.fr closed.\n",
      "\n",
      "2025-05-20 13:46:13,668 \u001b[36mWARNING:\u001b[m terminated: <SshProcess('singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; python3 /home/lmascare/bench/in-situ/bench_deisa.py 2 /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/analytics.e 2>&1\"', Host('dahu-9.grenoble.grid5000.fr'), name=singularity exec /home/lmascare/benc..., started=True, start_date=2025-05-20 13:37:43+02:00, ended=True, end_date=2025-05-20 13:46:13+02:00, killed=False, error=False, error_reason=None, timeouted=False, expect_fail=False, write_error=False, exit_code=65280, ok=False, pid=3486994, real cmd=('ssh', '-tt', '-o', 'BatchMode=yes', '-o', 'PasswordAuthentication=no', '-o', 'StrictHostKeyChecking=no', '-o', 'UserKnownHostsFile=/dev/null', '-o', 'ConnectTimeout=20', 'dahu-9.grenoble.grid5000.fr', 'singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; python3 /home/lmascare/bench/in-situ/bench_deisa.py 2 /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/analytics.e 2>&1\"'))>\n",
      "stdout:\n",
      "/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "\n",
      "stderr:\n",
      "Warning: Permanently added 'dahu-9.grenoble.grid5000.fr,172.16.20.9' (ECDSA) to the list of known hosts.\n",
      "Connection to dahu-9.grenoble.grid5000.fr closed by remote host.\n",
      "Connection to dahu-9.grenoble.grid5000.fr closed.\n",
      "\n",
      "2025-05-20 13:46:13,669 \u001b[36mWARNING:\u001b[m terminated: <SshProcess('singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"dask worker --nworkers 1 --nthreads 1 --local-directory /tmp --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/worker.e 2>&1\"', Host('dahu-9.grenoble.grid5000.fr'), name=singularity exec /home/lmascare/benc..., started=True, start_date=2025-05-20 13:37:27+02:00, ended=True, end_date=2025-05-20 13:46:13+02:00, killed=False, error=False, error_reason=None, timeouted=False, expect_fail=False, write_error=False, exit_code=65280, ok=False, pid=3486936, real cmd=('ssh', '-tt', '-o', 'BatchMode=yes', '-o', 'PasswordAuthentication=no', '-o', 'StrictHostKeyChecking=no', '-o', 'UserKnownHostsFile=/dev/null', '-o', 'ConnectTimeout=20', 'dahu-9.grenoble.grid5000.fr', 'singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"dask worker --nworkers 1 --nthreads 1 --local-directory /tmp --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/worker.e 2>&1\"'))>\n",
      "stdout:\n",
      "/usr/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\n",
      "\n",
      "stderr:\n",
      "Warning: Permanently added 'dahu-9.grenoble.grid5000.fr,172.16.20.9' (ECDSA) to the list of known hosts.\n",
      "Connection to dahu-9.grenoble.grid5000.fr closed by remote host.\n",
      "Connection to dahu-9.grenoble.grid5000.fr closed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import execo\n",
    "import execo_g5k\n",
    "import execo_engine\n",
    "import time\n",
    "import os\n",
    "\n",
    "# this will be /home/lmascare\n",
    "HOME_DIR = os.path.expanduser(\"~\")\n",
    "\n",
    "PATH_TO_SIF_FILE = HOME_DIR + \"/bench/docker/images/bench.sif\"\n",
    "\n",
    "SIMULATION_INI = HOME_DIR + \"/bench/simulation/setup.ini\"\n",
    "PDI_DEISA_YML = HOME_DIR + \"/bench/simulation/io_deisa.yml\"\n",
    "SIM_EXECUTABLE = HOME_DIR + \"/bench/simulation/build/main\"\n",
    "DEISA_PATH = HOME_DIR + \"/bench/deisa/\"\n",
    "\n",
    "ANALYTICS_PY_FILE = HOME_DIR + \"/bench/in-situ/bench_deisa.py\"\n",
    "\n",
    "SCHEDULER_FILE = HOME_DIR + \"/bench/experiment/scheduler.json\"\n",
    "\n",
    "OUTPUT_DIR = HOME_DIR + \"/bench/experiment/\"\n",
    "\n",
    "NDASKWORKERS = 2\n",
    "DASK_WORKERS_PER_NODE = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0db1f3-6229-4d37-9676-9795b2e9bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Get configs from a .ini file.\n",
    "\n",
    "Arguments:\n",
    "    config_file (str): Path to the .ini configuration file.\n",
    "\"\"\"\n",
    "def get_configs(config_file):\n",
    "    import configparser\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    # Assuming the .ini file has sections and keys\n",
    "    # Example: [section_name] key_name = value\n",
    "    configs = {}\n",
    "    for section in config.sections():\n",
    "        for key, value in config.items(section):\n",
    "            configs[key] = value\n",
    "\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bfd03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 2476681 reserved on site grenoble\n",
      "Head node: Host('dahu-9.grenoble.grid5000.fr')\n",
      "Other nodes: []\n",
      "Total simulation cores: 0\n"
     ]
    }
   ],
   "source": [
    "configs = get_configs(SIMULATION_INI)\n",
    "\n",
    "nb_reserved_nodes = 1\n",
    "\n",
    "jobs = execo_g5k.oarsub(\n",
    "    [\n",
    "        (\n",
    "            execo_g5k.OarSubmission(f\"nodes={nb_reserved_nodes}\", walltime=10*60),\n",
    "            \"grenoble\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_id, site = jobs[0]\n",
    "print(f\"Job {job_id} reserved on site {site}\")\n",
    "\n",
    "nodes = execo_g5k.oar.get_oar_job_nodes(job_id, site)\n",
    "head_node, nodes = nodes[0], nodes[1:]\n",
    "\n",
    "print(f\"Head node: {head_node}\")\n",
    "print(f\"Other nodes: {nodes}\")\n",
    "\n",
    "\n",
    "cores_per_node = execo_g5k.get_host_attributes(head_node)[\"architecture\"][\"nb_cores\"]\n",
    "total_simulation_cores = cores_per_node * len(nodes)\n",
    "\n",
    "print(f\"Total simulation cores: {total_simulation_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebf5ed13-361a-4915-815e-f6a234459527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask scheduler --host dahu-9.grenoble.grid5000.fr --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/scheduler.e 2>&1\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING SCHEDULER\n",
    "##########################################################\n",
    "# Sending the commands for head node\n",
    "\n",
    "if os.path.exists(SCHEDULER_FILE):\n",
    "    os.remove(SCHEDULER_FILE)\n",
    "\n",
    "#redirecting the output to a file\n",
    "scheduler_cmd = (\n",
    "    f\"dask scheduler \"\n",
    "    f\"--host {head_node.address} \"\n",
    "    f\"--scheduler-file {SCHEDULER_FILE} \"\n",
    "    # \"--port 8786 \"\n",
    "    f\"> {OUTPUT_DIR}scheduler.e 2>&1\"\n",
    ")\n",
    "print(scheduler_cmd)\n",
    "scheduler_process = execo.SshProcess(\n",
    "    f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{scheduler_cmd}\"',\n",
    "    head_node,\n",
    ")\n",
    "scheduler_process.start()\n",
    "\n",
    "# Wait for the scheduler to start by checking the scheduler file\n",
    "while not os.path.exists(SCHEDULER_FILE):\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c724658e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SshProcess('singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"dask worker --nworkers 1 --nthreads 1 --local-directory /tmp --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/worker.e 2>&1\"', Host('dahu-9.grenoble.grid5000.fr'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING WORKERS\n",
    "##########################################################\n",
    "\n",
    "# worker_cmd = (\n",
    "#     f\"dask worker tcp://{head_node.address}:8786 \"\n",
    "#     \"--nworkers {DASK_WORKERS_PER_NODE} \"\n",
    "#     \"--nthreads 1 \"\n",
    "#     \"--local-directory /tmp \"\n",
    "#     \"--scheduler-file={SCHEDULER_FILE} \"\n",
    "#     \"> {OUTPUT_DIR}worker.e 2>&1\"\n",
    "# )\n",
    "\n",
    "# # for node in nodes:\n",
    "#     # redirecting the output to a file\n",
    "# worker_process = execo.SshProcess(\n",
    "#     f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{worker_cmd}\"',\n",
    "#     nodes[0],\n",
    "# )\n",
    "# worker_process.start()\n",
    "\n",
    "worker_cmd = (\n",
    "    f\"dask worker \"\n",
    "    # \"tcp://{head_node.address}:8786 \"\n",
    "    f\"--nworkers {DASK_WORKERS_PER_NODE} \"\n",
    "    \"--nthreads 1 \"\n",
    "    \"--local-directory /tmp \"\n",
    "    f\"--scheduler-file {SCHEDULER_FILE} \"\n",
    "    f\"> {OUTPUT_DIR}worker.e 2>&1\"\n",
    ")\n",
    "\n",
    "# for node in nodes:\n",
    "    # redirecting the output to a file\n",
    "worker_process = execo.SshProcess(\n",
    "    f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{worker_cmd}\"',\n",
    "    head_node,\n",
    ")\n",
    "worker_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27d19c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics command:\n",
      "singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; python3 /home/lmascare/bench/in-situ/bench_deisa.py 2 /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/analytics.e 2>&1\"\n",
      "\n",
      "####################\n",
      "Output:\n",
      "\n",
      "--------------------\n",
      "Error:\n",
      "\n",
      "\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING ANALYTICS\n",
    "##########################################################\n",
    "\n",
    "# python3 $OPWD/in-situ/bench_deisa.py $NDASKWORKERS $SCHEFILE 2>analytics.e&\n",
    "\n",
    "py_cmd = (\n",
    "    'export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; '\n",
    "    f'python3 {ANALYTICS_PY_FILE} {NDASKWORKERS} {SCHEDULER_FILE} > {OUTPUT_DIR}analytics.e 2>&1'\n",
    ")\n",
    "analytics_cmd = (\n",
    "    # 'hostname'\n",
    "    f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{py_cmd}\"'\n",
    "\n",
    ")\n",
    "print(\"Analytics command:\")\n",
    "print(analytics_cmd)\n",
    "\n",
    "analytics_process = execo.SshProcess(\n",
    "    analytics_cmd,\n",
    "    head_node,\n",
    ")\n",
    "analytics_process.start()\n",
    "# analytics_process.wait()\n",
    "\n",
    "print()\n",
    "print(\"#\"*20)\n",
    "print(\"Output:\")\n",
    "print(analytics_process.stdout)\n",
    "print(\"-\"*20)\n",
    "print(\"Error:\")\n",
    "print(analytics_process.stderr)\n",
    "print()\n",
    "print(\"#\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0de3919f-dde4-4d5b-9f74-7f4c90833502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation with 4 MPI processes\n",
      "Simulation command:\n",
      "export OMP_NUM_THREADS=2; export OMP_PROC_BIND=spread; export OMP_PLACES=threads; mpirun -np 4 singularity exec /home/lmascare/bench/docker/images/bench.sif pdirun /home/lmascare/bench/simulation/build/main /home/lmascare/bench/simulation/setup.ini /home/lmascare/bench/simulation/io_deisa.yml --kokkos-map-device-id-by=mpi_rank > /home/lmascare/bench/experiment/simulation.e 2>&1\n",
      "Output:\n",
      "\n",
      "--------------------\n",
      "Error:\n",
      "Warning: Permanently added 'dahu-9.grenoble.grid5000.fr,172.16.20.9' (ECDSA) to the list of known hosts.\n",
      "Connection to dahu-9.grenoble.grid5000.fr closed.\n",
      "\n",
      "\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING SIMULATION\n",
    "##########################################################\n",
    "\n",
    "# Starting the simulation 32 - 30 for sim and 2 for dask \n",
    "\n",
    "mx = int(configs[\"mx\"])\n",
    "my = int(configs[\"my\"])\n",
    "mz = int(configs[\"mz\"])\n",
    "\n",
    "MPI_NP = mx * my * mz\n",
    "\n",
    "print(f\"Running simulation with {MPI_NP} MPI processes\")\n",
    "\n",
    "host_list = \",\".join([f\"{node.address}:{2}\" for node in nodes])\n",
    "simulation_cmd = (\n",
    "    # 'hostname'\n",
    "    f'pdirun {SIM_EXECUTABLE} {SIMULATION_INI} {PDI_DEISA_YML} --kokkos-map-device-id-by=mpi_rank > {OUTPUT_DIR}simulation.e 2>&1'\n",
    ")\n",
    "\n",
    "# Build the command with two singularity exec calls:\n",
    "mpi_cmd = (\n",
    "    'export OMP_NUM_THREADS=2; '\n",
    "    'export OMP_PROC_BIND=spread; '\n",
    "    'export OMP_PLACES=threads; '\n",
    "    f\"mpirun -np {MPI_NP} \"\n",
    "    # f\"--host {host_list} \"\n",
    "    f\"singularity exec {PATH_TO_SIF_FILE} {simulation_cmd}\"\n",
    ")\n",
    "\n",
    "print(\"Simulation command:\")\n",
    "print(mpi_cmd)\n",
    "mpi_process = execo.SshProcess(\n",
    "    mpi_cmd,\n",
    "    head_node,\n",
    ")\n",
    "mpi_process.start()\n",
    "\n",
    "mpi_process.wait()\n",
    "\n",
    "print(\"Output:\")\n",
    "print(mpi_process.stdout)\n",
    "print(\"-\"*20)\n",
    "print(\"Error:\")\n",
    "print(mpi_process.stderr)\n",
    "print()\n",
    "print(\"#\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65245631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
