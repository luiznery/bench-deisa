{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "684bbeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install dask==2024.6.0 distributed==2024.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "018ad10d-fcb5-41ce-8fd7-db8d62872dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import execo\n",
    "import execo_g5k\n",
    "import execo_engine\n",
    "import time\n",
    "import os\n",
    "from dask.distributed import Client\n",
    "\n",
    "# this will be /home/lmascare\n",
    "HOME_DIR = os.path.expanduser(\"~\")\n",
    "\n",
    "PATH_TO_SIF_FILE = HOME_DIR + \"/bench/docker/images/bench.sif\"\n",
    "\n",
    "SIMULATION_INI = HOME_DIR + \"/bench/simulation/setup.ini\"\n",
    "PDI_DEISA_YML = HOME_DIR + \"/bench/simulation/io_deisa.yml\"\n",
    "SIM_EXECUTABLE = HOME_DIR + \"/bench/simulation/build/main\"\n",
    "DEISA_PATH = HOME_DIR + \"/bench/deisa/\"\n",
    "\n",
    "ANALYTICS_PY_FILE = HOME_DIR + \"/bench/in-situ/bench_deisa.py\"\n",
    "\n",
    "# must be in the same directory as the script/notebook\n",
    "SCHEDULER_FILE = HOME_DIR + \"/bench/experiment/scheduler.json\"\n",
    "\n",
    "OUTPUT_DIR = HOME_DIR + \"/bench/experiment/\"\n",
    "\n",
    "# the total number of nodes needs to be THE SAME as the total number of workers passed in the analytics script\n",
    "DASK_WORKERS_PER_NODE = 2\n",
    "TOTAL_DASK_WORKERS = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0db1f3-6229-4d37-9676-9795b2e9bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Get configs from a .ini file.\n",
    "\n",
    "Arguments:\n",
    "    config_file (str): Path to the .ini configuration file.\n",
    "\"\"\"\n",
    "def get_configs(config_file):\n",
    "    import configparser\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    # Assuming the .ini file has sections and keys\n",
    "    # Example: [section_name] key_name = value\n",
    "    configs = {}\n",
    "    for section in config.sections():\n",
    "        for key, value in config.items(section):\n",
    "            configs[key] = value\n",
    "\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21bfd03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job 2476997 reserved on site grenoble\n",
      "Head node: Host('dahu-12.grenoble.grid5000.fr')\n",
      "Other nodes: []\n",
      "Total simulation cores: 0\n"
     ]
    }
   ],
   "source": [
    "configs = get_configs(SIMULATION_INI)\n",
    "\n",
    "nb_reserved_nodes = 1\n",
    "\n",
    "jobs = execo_g5k.oarsub(\n",
    "    [\n",
    "        (\n",
    "            execo_g5k.OarSubmission(f\"nodes={nb_reserved_nodes}\", walltime=60*60),\n",
    "            \"grenoble\",\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "job_id, site = jobs[0]\n",
    "print(f\"Job {job_id} reserved on site {site}\")\n",
    "\n",
    "nodes = execo_g5k.oar.get_oar_job_nodes(job_id, site)\n",
    "head_node, nodes = nodes[0], nodes[1:]\n",
    "\n",
    "print(f\"Head node: {head_node}\")\n",
    "print(f\"Other nodes: {nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cores_per_node = execo_g5k.get_host_attributes(head_node)[\"architecture\"][\"nb_cores\"]\n",
    "total_simulation_cores = cores_per_node * len(nodes)\n",
    "\n",
    "print(f\"Total simulation cores: {total_simulation_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30825f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head node IP: 172.16.20.12\n",
      "Other nodes IPs: []\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "head_node_ip = socket.gethostbyname(head_node.address)\n",
    "\n",
    "nodes_ips = []\n",
    "for node in nodes:\n",
    "    nodes_ips.append(socket.gethostbyname(node.address))\n",
    "print(f\"Head node IP: {head_node_ip}\")\n",
    "print(f\"Other nodes IPs: {nodes_ips}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebf5ed13-361a-4915-815e-f6a234459527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask scheduler --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/scheduler.e 2>&1; sync\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING SCHEDULER\n",
    "##########################################################\n",
    "# Sending the commands for head node\n",
    "\n",
    "if os.path.exists(SCHEDULER_FILE):\n",
    "    os.remove(SCHEDULER_FILE)\n",
    "\n",
    "#redirecting the output to a file\n",
    "scheduler_cmd = (\n",
    "    f\"dask scheduler \"\n",
    "    # f\"--host {head_node.address} \"\n",
    "    f\"--scheduler-file {SCHEDULER_FILE} \"\n",
    "    f\"> {OUTPUT_DIR}scheduler.e 2>&1; \"\n",
    "    \"sync\"\n",
    ")\n",
    "print(scheduler_cmd)\n",
    "scheduler_process = execo.SshProcess(\n",
    "    f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{scheduler_cmd}\"',\n",
    "    head_node,\n",
    ")\n",
    "scheduler_process.start()\n",
    "\n",
    "# Wait for the scheduler to start by checking the scheduler file\n",
    "while not os.path.exists(SCHEDULER_FILE):\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c724658e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SshProcess('singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"dask worker --nworkers 2 --nthreads 1 --local-directory /tmp --scheduler-file /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/worker.e 2>&1\"', Host('dahu-12.grenoble.grid5000.fr'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING WORKERS\n",
    "##########################################################\n",
    "\n",
    "worker_cmd = (\n",
    "    f\"dask worker \"\n",
    "    # f\"tcp://{head_node_ip}:8786 \"\n",
    "    # f\"--dashboard-address {head_node.address}:8787 \"\n",
    "    f\"--nworkers {DASK_WORKERS_PER_NODE} \"\n",
    "    \"--nthreads 1 \"\n",
    "    \"--local-directory /tmp \"\n",
    "    f\"--scheduler-file {SCHEDULER_FILE} \"\n",
    "    f\"> {OUTPUT_DIR}worker.e 2>&1\"\n",
    ")\n",
    "\n",
    "# for node in nodes:\n",
    "    # redirecting the output to a file\n",
    "worker_process = execo.SshProcess(\n",
    "    f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{worker_cmd}\"',\n",
    "    head_node,\n",
    ")\n",
    "worker_process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27d19c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytics command:\n",
      "singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; python3 /home/lmascare/bench/in-situ/bench_deisa.py 2 /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/analytics.e 2>&1\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SshProcess('singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; python3 /home/lmascare/bench/in-situ/bench_deisa.py 2 /home/lmascare/bench/experiment/scheduler.json > /home/lmascare/bench/experiment/analytics.e 2>&1\"', Host('dahu-12.grenoble.grid5000.fr'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING ANALYTICS\n",
    "##########################################################\n",
    "\n",
    "# python3 $OPWD/in-situ/bench_deisa.py $NDASKWORKERS $SCHEFILE 2>analytics.e&\n",
    "\n",
    "py_cmd = (\n",
    "    'export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; '\n",
    "    f'python3 {ANALYTICS_PY_FILE} {TOTAL_DASK_WORKERS} {SCHEDULER_FILE} > {OUTPUT_DIR}analytics.e 2>&1'\n",
    ")\n",
    "analytics_cmd = (\n",
    "    # 'hostname'\n",
    "    f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{py_cmd}\"'\n",
    "\n",
    ")\n",
    "print(\"Analytics command:\")\n",
    "print(analytics_cmd)\n",
    "\n",
    "analytics_process = execo.SshProcess(\n",
    "    analytics_cmd,\n",
    "    head_node,\n",
    ")\n",
    "analytics_process.start()\n",
    "# analytics_process.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0de3919f-dde4-4d5b-9f74-7f4c90833502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation with 4 MPI processes\n",
      "Simulation command:\n",
      "export OMP_NUM_THREADS=2; export OMP_PROC_BIND=spread; export OMP_PLACES=threads; mpirun -np 4 singularity exec /home/lmascare/bench/docker/images/bench.sif bash -c \"export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; pdirun /home/lmascare/bench/simulation/build/main /home/lmascare/bench/simulation/setup.ini /home/lmascare/bench/simulation/io_deisa.yml --kokkos-map-device-id-by=mpi_rank > /home/lmascare/bench/experiment/simulation.e 2>&1\"\n"
     ]
    }
   ],
   "source": [
    "##########################################################\n",
    "#                   RUNNING SIMULATION\n",
    "##########################################################\n",
    "\n",
    "# Starting the simulation 32 - 30 for sim and 2 for dask \n",
    "\n",
    "mx = int(configs[\"mx\"])\n",
    "my = int(configs[\"my\"])\n",
    "mz = int(configs[\"mz\"])\n",
    "\n",
    "MPI_NP = mx * my * mz\n",
    "\n",
    "print(f\"Running simulation with {MPI_NP} MPI processes\")\n",
    "\n",
    "host_list = \",\".join([f\"{node.address}:{2}\" for node in nodes])\n",
    "simulation_cmd = (\n",
    "    f'export PYTHONPATH=/home/lmascare/bench/deisa/:$PYTHONPATH; '\n",
    "    f'pdirun {SIM_EXECUTABLE} {SIMULATION_INI} {PDI_DEISA_YML} --kokkos-map-device-id-by=mpi_rank > {OUTPUT_DIR}simulation.e 2>&1'\n",
    ")\n",
    "\n",
    "# Build the command with two singularity exec calls:\n",
    "mpi_cmd = (\n",
    "    'export OMP_NUM_THREADS=2; '\n",
    "    'export OMP_PROC_BIND=spread; '\n",
    "    'export OMP_PLACES=threads; '\n",
    "    f\"mpirun -np {MPI_NP} \"\n",
    "    # f\"--host {host_list} \"\n",
    "    f'singularity exec {PATH_TO_SIF_FILE} bash -c \"{simulation_cmd}\"'\n",
    ")\n",
    "\n",
    "print(\"Simulation command:\")\n",
    "print(mpi_cmd)\n",
    "mpi_process = execo.SshProcess(\n",
    "    mpi_cmd,\n",
    "    head_node,\n",
    ")\n",
    "mpi_process.start()\n",
    "\n",
    "time.sleep(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dac75a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-jupyter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
